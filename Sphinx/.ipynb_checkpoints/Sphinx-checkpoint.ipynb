{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR: Sphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import argparse\n",
    "import csv\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './'\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "target_unpacked_dir = os.path.join(target_dir, \"CV_unpacked\")\n",
    "os.makedirs(target_unpacked_dir, exist_ok=True)\n",
    "\n",
    "if args.tar_path and os.path.exists(args.tar_path):\n",
    "    print('Find existing file {}'.format(args.tar_path))\n",
    "    target_file = args.tar_path\n",
    "else:\n",
    "    print(\"Could not find downloaded Common Voice archive, Downloading corpus...\")\n",
    "    filename = wget.download(COMMON_VOICE_URL, target_dir)\n",
    "    target_file = os.path.join(target_dir, os.path.basename(filename))\n",
    "\n",
    "print(\"Unpacking corpus to {} ...\".format(target_unpacked_dir))\n",
    "tar = tarfile.open(target_file)\n",
    "tar.extractall(target_unpacked_dir)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "1. Create ID files \n",
    "2. Create Transcription files\n",
    "3. Convert to wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = os.path.join(target_dir, 'wav/')\n",
    "etc_dir = os.path.join(target_dir, 'etc/')\n",
    "dataset_name = os.path.basename(target_dir)\n",
    "os.makedirs(wav_dir, exist_ok=True)\n",
    "os.makedirs(etc_dir, exist_ok=True)\n",
    "path_to_data = os.path.dirname(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fileids(csv_file, target_dir, dataset_type): \n",
    "    print('Creating fileids file for {}.'.format(csv_file))\n",
    "    with open(csv_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        data = []\n",
    "        with open(os.path.join(etc_dir, dataset_name + '_'+ dataset_type + '.fileids'), 'w') as ids_file:\n",
    "            for row in reader: \n",
    "                file_name = row['filename']\n",
    "                wav_path = os.path.join(wav_dir, dataset_type + '_'+ os.path.splitext(os.path.basename(file_name))[0] + '.wav')\n",
    "                ids_file.write(os.path.basename(wav_path)[0] + '\\n')\n",
    "\n",
    "                data.append((file_name,wav_path))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_files(csv_file, target_dir, dataset_type): \n",
    "    print('Creating transcript files for {}.'.format(csv_file))\n",
    "    with open(csv_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        with open(os.path.join(etc_dir, dataset_name + '_'+ dataset_type + '.transcription'), 'w') as trans_file:\n",
    "            for row in reader: \n",
    "                file_name = row['filename']\n",
    "                wav_path = os.path.join(wav_dir, dataset_type + '_'+ os.path.splitext(os.path.basename(file_name))[0] + '.wav')\n",
    "                trans_file.write('<s> '+ row['text'] + ' </s> (' + os.path.basename(wav_path)[0] + ')' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(x):\n",
    "    file_path, wav_path = x\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    cmd = \"sox {} -r {} -b 16 -c 1 {}\".format(\n",
    "        os.path.join(path_to_data, file_path),\n",
    "        args.sample_rate,\n",
    "        wav_path)\n",
    "    subprocess.call([cmd], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the training data\n",
    "train_csv_file = os.path.join(target_unpacked_dir, 'cv_corpus_v1/', 'cv-valid-train.csv'\n",
    "train_wav_files = create_fileids(train_csv_file,target_dir, 'train')\n",
    "create_transcript_files(train_csv_file,target_dir, 'train')\n",
    "with ThreadPool(10) as pool:\n",
    "    pool.map(convert_to_wav, train_wav_files)\n",
    "\n",
    "# Format the validation data\n",
    "val_csv_file = os.path.join(target_unpacked_dir, 'cv_corpus_v1/', 'cv-valid-dev.csv')\n",
    "val_wav_files = create_fileids(val_csv_file, target_dir, 'test')\n",
    "create_transcript_files(val_csv_file, target_dir, 'test')\n",
    "with ThreadPool(10) as pool:\n",
    "    pool.map(convert_to_wav, val_wav_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word list from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "counter = collections.Counter()\n",
    "with open(csv_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader: \n",
    "            trans = row['text']\n",
    "            counter += collections.Counter(trans.split())\n",
    "\n",
    "with open(os.path.join(etc_dir,'common_voice.words'), 'w') as f: \n",
    "    for item in counter:\n",
    "        f.write(item.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create phonetic dictionary (lexicon) \n",
    "The phone list is the list of phones that will be used in the model. \n",
    "A default phone list is provided. \n",
    "\n",
    "A lexicon/phonetic dictionary can be obtained for each of the  words in the dataset by using: [CMU Lextool](http://www.speech.cs.cmu.edu/tools/lextool.html)\n",
    "\n",
    "We provide one, to prevent some of the necessary formatting (convert to lower case, phone replacement to match phone list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat etc/common_voice.phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head etc/common_voice.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls etc/common_voice_train_transcript_only.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create vocab file\n",
    "text2wfreq < etc/common_voice_train_transcript_only.txt | wfreq2vocab > etc/common_voice.vocab\n",
    "\n",
    "# # Create n-gram count from training transcript file\n",
    "text2idngram -vocab etc/common_voice.vocab -idngram etc/common_voice.idngram < etc/common_voice_train.transcription \n",
    "\n",
    "# # Create language model from n-grams\n",
    "idngram2lm -vocab_type 0 -idngram etc/common_voice.idngram -vocab etc/common_voice.vocab -arpa etc/common_voice.lm\n",
    "\n",
    "# # Convert language model to DMP format\n",
    "!sphinx_lm_convert -i etc/common_voice.lm -o etc/common_voice.lm.DMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sphinx_lm_convert -i etc/common_voice.lm -o etc/common_voice.dmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head etc/common_voice.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing complete\n",
    "Make sure we have all the files necessary.\n",
    "```\n",
    "$ls etc/\n",
    "\n",
    "common_voice.dic      common_voice.vocab\n",
    "common_voice.filler   common_voice_test.fileids\n",
    "common_voice.idngram  common_voice_test.transcription\n",
    "common_voice.lm       common_voice_train.fileids\n",
    "common_voice.lm.bin   common_voice_train.transcription\n",
    "common_voice.phone\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls etc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Setup \n",
    "Creates the config for the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sphinxtrain -t common_voice setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sphinx Model\n",
    "Becasue the Sphinx is CPU-based, the training on a large dataset can take a very long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess, time, os, sys\n",
    "cmd = [\"sphinxtrain\",\"run\"]\n",
    "\n",
    "p = subprocess.Popen(cmd,\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.STDOUT)\n",
    "\n",
    "for line in iter(p.stdout.readline, b''):\n",
    "    print(\">>> \" + line.decode().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sphinx_lm_convert -i etc/common_voice.lm -o etc/common_voice.lm.DMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to change in config:\n",
    "$CFG_CD_TRAIN = 'no';\n",
    "$DEC_CFG_MODEL_NAME = \"$CFG_EXPTNAME.ci_cont\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, os, sys\n",
    "cmd = [\"sphinxtrain\",\"-s\", \"decode\", \"run\"]\n",
    "\n",
    "p = subprocess.Popen(cmd,\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.STDOUT)\n",
    "\n",
    "for line in iter(p.stdout.readline, b''):\n",
    "    print(\">>> \" + line.decode().rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pocketsphinx_batch  -adcin yes  -cepdir test/  -cepext .wav  -ctl test.fileids -hmm model_parameters/common_voice.ci_cont/ -lm etc/common_voice.lm.DMP -dict etc/common_voice.dic -hyp predictions.hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "    s1 (string): space-separated sentence\n",
    "    s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "    \n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    wer_lev = Lev.distance(''.join(w1), ''.join(w2))\n",
    "    wer_inst = float(wer_lev)/len(s1.split()) * 100\n",
    "    return 'WER: {0:.2f}'.format(wer_inst)\n",
    "\n",
    "def cer(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "    Arguments:\n",
    "    s1 (string): space-separated sentence\n",
    "    s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "    cer_inst = float(Lev.distance(s1, s2)) / len(s1) * 100\n",
    "    return 'CER: {0:.2f}'.format(cer_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "def word_lev(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def char_lev(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "    return Lev.distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target_data = pd.read_csv('cv-valid-test.csv')\n",
    "predicted_data = pd.read_csv('predictions.hyp', names=['text'], header=None)\n",
    "predicted_data['text'] = predicted_data['text'].str.replace(r\"\\(.*\\)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(str(wer(predicted_data['text'][index], target_data['text'][index])))\n",
    "print('Hypothesis: '+ predicted_data['text'][index] +'\\nTarget: ' + target_data['text'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "verbose = True\n",
    "\n",
    "for index in range(len(target_data)):\n",
    "    transcript, reference = predicted_data['text'][index], target_data['text'][index]\n",
    "    wer_inst = word_lev(transcript, reference)\n",
    "    cer_inst = char_lev(transcript, reference)\n",
    "    total_wer += wer_inst\n",
    "    total_cer += cer_inst\n",
    "    num_tokens += len(reference.split())\n",
    "    num_chars += len(reference)\n",
    "    if verbose:\n",
    "        print(\"Ref:\", reference.lower())\n",
    "        print(\"Hyp:\", transcript.lower())\n",
    "        print(\"WER:\", 100*float(wer_inst) / len(reference.split()), \"CER:\", 100*float(cer_inst) / len(reference), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = float(total_wer) / num_tokens\n",
    "cer = float(total_cer) / num_chars\n",
    "\n",
    "print('Test Summary \\t'\n",
    "      'Average WER {wer:.3f}\\t'\n",
    "      'Average CER {cer:.3f}\\t'.format(wer=wer * 100, cer=cer * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
